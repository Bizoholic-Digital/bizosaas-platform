# BizOSaaS Platform - Complete Implementation Plan
## Domain-Driven Design Microservices with 93+ CrewAI Agents

**Version:** 2.0 FINAL
**Date:** November 14, 2025
**Status:** üìã APPROVED - READY FOR IMPLEMENTATION
**Duration:** 21 Days (3 Weeks)
**Server:** KVM4 (72.60.219.244) - Hostinger VPS
**Dokploy:** https://dk4.bizoholic.com

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Current State Analysis](#current-state-analysis)
3. [Architecture Vision](#architecture-vision)
4. [Implementation Phases](#implementation-phases)
   - [Phase 0: Emergency Fixes](#phase-0-emergency-fixes-day-1)
   - [Phase 1: API Gateway](#phase-1-api-gateway--service-exposure-days-2-4)
   - [Phase 2: CrewAI Orchestration](#phase-2-crewai-agents-orchestration-days-5-8)
   - [Phase 3: Centralized Auth](#phase-3-centralized-authentication--authorization-days-9-11)
   - [Phase 4: DDD Boundaries](#phase-4-ddd-service-boundaries--modularization-days-12-15)
   - [Phase 5: Containerization](#phase-5-containerization--deployment-automation-days-16-18)
   - [Phase 6: Testing & Docs](#phase-6-comprehensive-testing--documentation-days-19-21)
5. [Success Metrics](#success-metrics)
6. [Deployment Checklist](#deployment-checklist)

---

## Executive Summary

### Mission

Transform the BizOSaaS platform from a partially integrated system into a production-ready, enterprise-grade SaaS platform with:
- **22 microservices** properly containerized and orchestrated
- **93+ CrewAI AI agents** autonomously managing operations
- **Human-In-The-Loop (HITL)** workflows for critical decisions
- **Domain-Driven Design (DDD)** architecture
- **Event-driven communication** for loose coupling
- **Zero-downtime deployments** via CI/CD

### Critical Issues Addressed

1. **üî¥ Client Portal Redirect Bug** - Users cannot access dashboard after login
2. **üî¥ Backend API Isolation** - 10 backend services have NO external access
3. **üî¥ Missing CrewAI Integration** - 93+ agents exist but not orchestrated
4. **‚ö†Ô∏è No HITL Workflows** - Critical decisions lack human oversight
5. **‚ö†Ô∏è Inconsistent Architecture** - Violates DDD and microservices principles

### Expected Outcomes

**Before Implementation:**
- 6/22 services accessible (27%)
- No AI agent orchestration
- Manual deployments only
- Single-tenant authentication
- Tight service coupling

**After Implementation:**
- 22/22 services accessible (100%)
- 93+ AI agents fully orchestrated
- Automated CI/CD deployments
- Multi-tenant JWT authentication
- Event-driven loose coupling

---

## Current State Analysis

### Service Inventory

#### Frontend Services (8 Applications)

| Service | Status | URL | Container Port | Image |
|---------|--------|-----|---------------|-------|
| client-portal | ‚úÖ Deployed (BROKEN) | `stg.bizoholic.com/portal` | 3002 | `bizosaas-client-portal:v2.2.9` |
| bizoholic-frontend | ‚úÖ Deployed | `stg.bizoholic.com/` | 3001 | `bizosaas-bizoholic-frontend:v3.1.3` |
| business-directory | ‚úÖ Deployed | `stg.bizoholic.com/directory` | 3004 | `bizosaas-business-directory:latest` |
| saleor-dashboard | ‚úÖ Deployed | `stg.coreldove.com/dashboard` | 80 | `saleor/saleor-dashboard:latest` |
| coreldove-frontend | ‚úÖ Deployed | `stg.coreldove.com/` | 3002 | `coreldove-storefront:latest` |
| thrillring-gaming | ‚úÖ Deployed | `stg.thrillring.com/` | 3006 | `bizosaas-thrillring-gaming:v1.0.7` |
| admin-dashboard | ‚ùå NO DOMAIN | N/A | N/A | `bizosaas-admin-dashboard:staging` |
| CoreLDove Storefront v2o | ‚ùå NO DOMAIN | N/A | N/A | GitHub source |

**Critical Issue:** Client portal redirects from `/portal/login` to `/dashboard` instead of `/portal/dashboard`

#### Backend Services (10 Applications)

| Service | Port | Current Domain | Purpose | Status |
|---------|------|----------------|---------|--------|
| brain-gateway | 8001 | ‚ùå NONE | Central AI Router | üî¥ Not Accessible |
| auth-service | 8002 | ‚ùå NONE | Authentication/SSO | üî¥ Not Accessible |
| django-crm | 8003 | ‚ùå NONE | CRM Backend | üî¥ Not Accessible |
| wagtail-cms | 8004 | ‚ùå NONE | Headless CMS | üî¥ Not Accessible |
| business-directory | 8005 | ‚ùå NONE | Directory Backend | üî¥ Not Accessible |
| coreldove-backend | 8006 | ‚ùå NONE | CoreLDove API | üî¥ Not Accessible |
| saleor-api | 8000 | ‚ùå NONE | E-commerce GraphQL | üî¥ Not Accessible |
| ai-agents | 8008 | ‚ùå NONE | AI/ML Services | üî¥ Not Accessible |
| quanttrade-backend | 8009 | ‚ùå NONE | Trading Backend | üî¥ Not Accessible |
| amazon-sourcing | 8010 | ‚ùå NONE | Amazon SP-API | üî¥ Not Accessible |

**Critical Issue:** ALL backend services are only accessible via internal Docker network

#### Infrastructure Services (4 Applications + 4 Databases)

| Service | URL | Status | Purpose |
|---------|-----|--------|---------|
| temporal-ui | `stg.bizoholic.com/temporal` | ‚úÖ Configured | Workflow UI |
| temporal-server | Internal Only | üü° Running | Workflow Engine |
| vault | ‚ùå NO DOMAIN | üî¥ Needs Config | Secrets Management |
| superset | ‚ùå NO DOMAIN | üî¥ Needs Config | Analytics |
| shared-postgres | Internal Only | ‚úÖ Running | Main Database |
| saleor-postgres | Internal Only | ‚úÖ Running | Saleor DB |
| shared-redis | Internal Only | ‚úÖ Running | Cache/Sessions |
| saleor-redis | Internal Only | ‚úÖ Running | Saleor Cache |

#### Missing Infrastructure

| Service | Purpose | Status |
|---------|---------|--------|
| RabbitMQ | Task queue for AI agents | ‚ùå NOT DEPLOYED |
| Kafka | Event streaming | ‚ùå NOT DEPLOYED |
| Agent Workers | 93+ CrewAI agents | ‚ùå NOT DEPLOYED |
| API Gateway | Unified routing | ‚ùå NOT DEPLOYED |

### Architecture Violations

#### 1. DDD Principle Violations

**Current State:**
- Services share authentication logic (tight coupling)
- No clear bounded contexts
- Direct service-to-service calls
- Shared database schemas between services

**Required:**
- Each service owns its domain
- Clear bounded contexts defined
- Event-driven communication
- Database per service pattern

#### 2. Microservices Anti-Patterns

**Current State:**
- No API gateway (frontends call backends directly)
- No circuit breakers (cascading failures possible)
- No service mesh
- Synchronous calls everywhere

**Required:**
- API Gateway for routing
- Circuit breakers for resilience
- Event bus for async communication
- Service mesh for observability

#### 3. CrewAI Integration Missing

**Current State:**
- 93+ agents exist in brain-gateway code
- No task queue (agents can't scale)
- No HITL workflow (no human approval)
- No event streaming (agents can't learn)

**Required:**
- RabbitMQ for task queuing
- Redis for real-time coordination
- Kafka for event streaming
- HITL API endpoints for approvals

---

## Architecture Vision

### Target Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        INTERNET / USERS                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   CLOUDFLARE CDN + WAF                              ‚îÇ
‚îÇ               (DNS: *.bizoholic.com, *.coreldove.com)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 TRAEFIK (Reverse Proxy + Load Balancer)             ‚îÇ
‚îÇ                    Server: KVM4 (72.60.219.244)                     ‚îÇ
‚îÇ                    Let's Encrypt SSL Auto-Renewal                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ             ‚îÇ              ‚îÇ               ‚îÇ
    ‚Üì             ‚Üì              ‚Üì               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Frontend  ‚îÇ ‚îÇ   API    ‚îÇ ‚îÇ  Admin   ‚îÇ  ‚îÇ    CMS     ‚îÇ
‚îÇ Services  ‚îÇ ‚îÇ Gateway  ‚îÇ ‚îÇ Services ‚îÇ  ‚îÇ  Services  ‚îÇ
‚îÇ  (7 apps) ‚îÇ ‚îÇ  (8080)  ‚îÇ ‚îÇ          ‚îÇ  ‚îÇ            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                  ‚îÇ                  ‚îÇ
        ‚Üì                  ‚Üì                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Brain Gateway ‚îÇ   ‚îÇ Auth Service ‚îÇ   ‚îÇ CRM Service ‚îÇ
‚îÇ    (8001)     ‚îÇ   ‚îÇ    (8002)    ‚îÇ   ‚îÇ   (8003)    ‚îÇ
‚îÇ               ‚îÇ   ‚îÇ              ‚îÇ   ‚îÇ             ‚îÇ
‚îÇ  - AI Router  ‚îÇ   ‚îÇ - Multi-     ‚îÇ   ‚îÇ - Leads     ‚îÇ
‚îÇ  - Agent      ‚îÇ   ‚îÇ   Tenant     ‚îÇ   ‚îÇ - Campaigns ‚îÇ
‚îÇ    Coord      ‚îÇ   ‚îÇ - JWT Auth   ‚îÇ   ‚îÇ - Contacts  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       MESSAGE INFRASTRUCTURE                        ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   RABBITMQ      ‚îÇ    ‚îÇ  REDIS STREAMS   ‚îÇ   ‚îÇ     KAFKA      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ (Task Queues)   ‚îÇ    ‚îÇ  (Real-time)     ‚îÇ   ‚îÇ (Event Stream) ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ    ‚îÇ                  ‚îÇ   ‚îÇ                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - auto_orders   ‚îÇ    ‚îÇ  - Agent Status  ‚îÇ   ‚îÇ - Audit Trail  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - auto_support  ‚îÇ    ‚îÇ  - HITL Queue    ‚îÇ   ‚îÇ - AI Learning  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - hitl_approval ‚îÇ    ‚îÇ  - Coordination  ‚îÇ   ‚îÇ - Analytics    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  93+ CREWAI AGENT WORKERS                           ‚îÇ
‚îÇ                  (Distributed Worker Pool)                          ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Order Agents (√ó4)    Support Agents (√ó6)    Inventory Agents (√ó3)  ‚îÇ
‚îÇ  Marketing (√ó4)       SEO (√ó10)              Social Media (√ó8)      ‚îÇ
‚îÇ  Analytics (√ó11)      Content (√ó5)           ... (√ó42 more)         ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  HITL Coordinator (√ó1) ‚Üí Escalates to humans when needed            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATA PERSISTENCE LAYER                           ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  PostgreSQL (√ó2)     Redis (√ó2)     MinIO S3     Vault              ‚îÇ
‚îÇ  - Shared DB         - Cache        - Media      - Secrets          ‚îÇ
‚îÇ  - Saleor DB         - Sessions     - Backups    - API Keys         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Domain-Driven Design Bounded Contexts

```
1. IDENTITY & ACCESS CONTEXT
   Service: auth-service (8002)
   Domain: Users, Tenants, Roles, Permissions
   Responsibility: Authentication, Authorization, Multi-tenancy
   Events Published: UserLoggedIn, TenantSwitched, PermissionGranted

2. CUSTOMER RELATIONSHIP CONTEXT
   Service: django-crm (8003)
   Domain: Clients, Leads, Campaigns, Communications
   Responsibility: Lead management, Customer engagement
   Events Published: LeadCreated, LeadConverted, CampaignLaunched

3. CONTENT MANAGEMENT CONTEXT
   Service: wagtail-cms (8004)
   Domain: Pages, Articles, Media, Menus
   Responsibility: Website content, Blog management
   Events Published: PagePublished, ArticleCreated, MediaUploaded

4. E-COMMERCE CONTEXT
   Service: saleor-api (8000)
   Domain: Products, Orders, Payments, Shipping
   Responsibility: Product catalog, Order processing
   Events Published: OrderPlaced, PaymentProcessed, ProductUpdated

5. AI ORCHESTRATION CONTEXT
   Service: brain-gateway (8001)
   Domain: Agents, Tasks, Workflows, Decisions
   Responsibility: AI coordination, Task routing, HITL
   Events Published: TaskAssigned, AgentCompleted, HITLRequested

6. BUSINESS DIRECTORY CONTEXT
   Service: business-directory (8005)
   Domain: Businesses, Categories, Reviews, Locations
   Responsibility: Local business listings, SEO
   Events Published: BusinessListed, ReviewPosted, LocationVerified

7. SOURCING CONTEXT
   Service: amazon-sourcing (8010)
   Domain: Products, Suppliers, Pricing, Inventory
   Responsibility: Product sourcing, Price monitoring
   Events Published: PriceChanged, StockUpdated, SupplierFound

8. TRADING CONTEXT
   Service: quanttrade-backend (8009)
   Domain: Portfolios, Trades, Strategies, Signals
   Responsibility: Trading algorithms, Risk management
   Events Published: TradeExecuted, SignalGenerated, RiskAlert
```

---

## Implementation Phases

## PHASE 0: Emergency Fixes (Day 1)

### Objective
Fix critical client portal redirect bug blocking user access

### Current Problem

```
User Flow (BROKEN):
1. Visit https://stg.bizoholic.com/portal
2. Redirect to /portal/login ‚úÖ
3. Enter credentials (demo@bizosaas.com / demo123)
4. Redirect to /dashboard ‚ùå WRONG!
5. 404 Error - page not found
6. Sidebar not visible, PWA broken

Expected Flow (CORRECT):
1-3. Same as above
4. Redirect to /portal/dashboard ‚úÖ
5. Dashboard loads, sidebar visible
6. PWA manifest loads from /portal/manifest.json
```

### Root Cause Analysis

**File:** `/src/app/(auth)/login/page.tsx`

```typescript
// CURRENT CODE (BROKEN):
const handleSubmit = async (e: React.FormEvent) => {
  e.preventDefault()
  setError('')

  try {
    const result = await login({ email, password })

    if (result.success) {
      // ‚ùå WRONG: Hardcoded redirect ignoring basePath
      window.location.href = returnUrl  // returnUrl defaults to '/dashboard'
    }
  } catch (err: any) {
    setError(err.message || 'Login failed')
  }
}
```

**Issue:** The `returnUrl` default value is `/dashboard` but should be `/portal/dashboard` to respect the basePath configuration.

### Solution Implementation

#### Step 1: Fix Login Page Redirect

**File:** `/home/alagiri/projects/bizosaas-platform/bizosaas/frontend/apps/client-portal/src/app/(auth)/login/page.tsx`

```typescript
'use client'

import { useState } from 'react'
import { useRouter, useSearchParams } from 'next/navigation'
import { login } from '@/lib/auth'

export default function LoginPage() {
  const router = useRouter()
  const searchParams = useSearchParams()
  const [email, setEmail] = useState('')
  const [password, setPassword] = useState('')
  const [error, setError] = useState('')

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault()
    setError('')

    try {
      const result = await login({ email, password })

      if (result.success) {
        // ‚úÖ FIX 1: Use router.push() which respects basePath
        const returnUrl = searchParams.get('returnUrl') || '/dashboard'
        router.push(returnUrl)  // Next.js router automatically adds basePath

        // Alternative FIX 2: Explicitly add basePath
        // window.location.href = `/portal${returnUrl}`
      } else {
        setError(result.error || 'Login failed. Please check your credentials.')
      }
    } catch (err: any) {
      setError(err.message || 'Login failed. Please check your credentials.')
    }
  }

  return (
    // ... rest of component
  )
}
```

#### Step 2: Verify Middleware Configuration

**File:** `/home/alagiri/projects/bizosaas-platform/bizosaas/frontend/apps/client-portal/src/middleware.ts`

```typescript
import { NextResponse } from 'next/server'
import type { NextRequest } from 'next/server'

export function middleware(request: NextRequest) {
  const { pathname } = request.nextUrl

  // Check if user is authenticated (check for token in cookies)
  const token = request.cookies.get('access_token')

  // Public routes (no auth required)
  const publicRoutes = ['/login', '/signup', '/forgot-password']
  const isPublicRoute = publicRoutes.some(route => pathname.endsWith(route))

  // Redirect unauthenticated users to login
  if (!token && !isPublicRoute && pathname.includes('/dashboard')) {
    const url = request.nextUrl.clone()
    url.pathname = '/login'  // basePath auto-added
    url.searchParams.set('returnUrl', pathname)
    return NextResponse.redirect(url)
  }

  // Redirect authenticated users away from login page
  if (token && isPublicRoute) {
    const url = request.nextUrl.clone()
    url.pathname = '/dashboard'  // ‚úÖ basePath auto-added = /portal/dashboard
    return NextResponse.redirect(url)
  }

  return NextResponse.next()
}

export const config = {
  matcher: [
    /*
     * Match all request paths except:
     * - api (API routes)
     * - _next/static (static files)
     * - _next/image (image optimization files)
     * - favicon.ico (favicon file)
     * - public files (public folder)
     */
    '/((?!api|_next/static|_next/image|favicon.ico|.*\\..*|manifest.json).*)',
  ],
}
```

#### Step 3: Verify Next.js Configuration

**File:** `/home/alagiri/projects/bizosaas-platform/bizosaas/frontend/apps/client-portal/next.config.js`

```javascript
/** @type {import('next').NextConfig} */
const nextConfig = {
  basePath: '/portal',  // ‚úÖ Ensure this is set
  output: 'standalone',

  // Ensure redirects respect basePath
  async redirects() {
    return [
      {
        source: '/',
        destination: '/dashboard',
        permanent: false,
      },
    ]
  },

  // ... rest of config
}

module.exports = nextConfig
```

### Build and Deploy v2.2.10

#### Build Script

```bash
#!/bin/bash
# /home/alagiri/projects/bizosaas-platform/scripts/deploy-client-portal-v2.2.10.sh

set -e

echo "üî® Building Client Portal v2.2.10 with redirect fix..."

cd /home/alagiri/projects/bizosaas-platform/bizosaas/frontend/apps/client-portal

# Build Docker image
docker build \
  --build-arg BASE_PATH=/portal \
  --build-arg NEXT_PUBLIC_API_URL=https://api.bizoholic.com/api \
  --build-arg NEXT_PUBLIC_BRAIN_GATEWAY_URL=https://api.bizoholic.com/api \
  --build-arg NEXT_PUBLIC_AUTH_API_URL=https://api.bizoholic.com/api/auth \
  --build-arg NEXT_PUBLIC_APP_URL=https://stg.bizoholic.com/portal \
  -t ghcr.io/bizoholic-digital/bizosaas-client-portal:v2.2.10 \
  -t ghcr.io/bizoholic-digital/bizosaas-client-portal:staging-latest \
  -f Dockerfile .

echo "üì§ Pushing to GitHub Container Registry..."

docker push ghcr.io/bizoholic-digital/bizosaas-client-portal:v2.2.10
docker push ghcr.io/bizoholic-digital/bizosaas-client-portal:staging-latest

echo "üöÄ Deploying to KVM4 via Dokploy API..."

curl -X POST "https://dk4.bizoholic.com/api/application.redeploy" \
  -H "accept: application/json" \
  -H "x-api-key: dk4ixNgzxiGcZWrjlvocbOJqTLjlZsJUEgmTJjjXYvLVSwiUBUPARxklyNFyVQRDHBa" \
  -H "Content-Type: application/json" \
  -d '{"applicationId":"8EqZXZKYTLiPqTkLF2l4J"}'

echo "‚úÖ Deployment triggered. Waiting for health check..."

sleep 30

# Verify deployment
HEALTH_CHECK=$(curl -s https://stg.bizoholic.com/portal/api/health || echo "failed")

if [[ $HEALTH_CHECK == *"healthy"* ]]; then
  echo "‚úÖ Client Portal v2.2.10 deployed successfully!"
  echo "üåê URL: https://stg.bizoholic.com/portal"
  echo "üîê Test: demo@bizosaas.com / demo123"
else
  echo "‚ùå Health check failed. Check deployment logs."
  exit 1
fi
```

### Testing Checklist

```bash
# Test Plan for v2.2.10

‚úÖ Test 1: Login Flow
1. Visit https://stg.bizoholic.com/portal
2. Should redirect to https://stg.bizoholic.com/portal/login
3. Login with demo@bizosaas.com / demo123
4. Should redirect to https://stg.bizoholic.com/portal/dashboard
5. Dashboard should load (not 404)

‚úÖ Test 2: Sidebar Visibility
1. After login, sidebar should be visible on left (desktop)
2. All 17 navigation items should be listed
3. Active page should be highlighted

‚úÖ Test 3: PWA Manifest
1. Open browser DevTools ‚Üí Application tab
2. Check Manifest section
3. Should load from https://stg.bizoholic.com/portal/manifest.json
4. No 404 errors

‚úÖ Test 4: Mobile Responsiveness
1. Resize browser to mobile size (<1024px)
2. Hamburger menu icon should appear
3. Click hamburger to open sidebar
4. Sidebar should overlay screen

‚úÖ Test 5: Direct Dashboard Access
1. Visit https://stg.bizoholic.com/portal/dashboard directly
2. If not logged in, should redirect to /portal/login?returnUrl=/dashboard
3. After login, should return to /portal/dashboard
```

### Success Criteria

- ‚úÖ Login redirects to `/portal/dashboard` (not `/dashboard`)
- ‚úÖ Sidebar visible with all 17 navigation items
- ‚úÖ PWA manifest loads from `/portal/manifest.json`
- ‚úÖ Mobile hamburger menu functional
- ‚úÖ No 404 errors in browser console
- ‚úÖ All navigation links work correctly

### Rollback Plan

If v2.2.10 has issues:

```bash
# Rollback to v2.2.9
curl -X POST "https://dk4.bizoholic.com/api/application.redeploy" \
  -H "x-api-key: dk4ixNgzxiGcZWrjlvocbOJqTLjlZsJUEgmTJjjXYvLVSwiUBUPARxklyNFyVQRDHBa" \
  -H "Content-Type: application/json" \
  -d '{
    "applicationId":"8EqZXZKYTLiPqTkLF2l4J",
    "imageTag":"v2.2.9"
  }'
```

### Time Estimate
**Duration:** 2-4 hours (including testing)

---

## PHASE 1: API Gateway & Service Exposure (Days 2-4)

### Objective
Expose all backend services via unified API gateway with proper routing

### Architecture Pattern

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         FRONTEND APPLICATIONS                    ‚îÇ
‚îÇ  (Client Portal, Admin, Marketing Sites)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ HTTPS Requests
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         TRAEFIK (Reverse Proxy)                  ‚îÇ
‚îÇ         dk4.bizoholic.com:443                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                      ‚îÇ
     ‚Üì                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  api.bizoholic  ‚îÇ   ‚îÇ  api.coreldove.com         ‚îÇ
‚îÇ      .com       ‚îÇ   ‚îÇ                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                     ‚îÇ
     ‚Üì                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          API GATEWAY SERVICE (8080)              ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  Route Table:                                    ‚îÇ
‚îÇ  ‚îú‚îÄ /auth/*      ‚Üí auth-service (8002)           ‚îÇ
‚îÇ  ‚îú‚îÄ /crm/*       ‚Üí django-crm (8003)             ‚îÇ
‚îÇ  ‚îú‚îÄ /cms/*       ‚Üí wagtail-cms (8004)            ‚îÇ
‚îÇ  ‚îú‚îÄ /directory/* ‚Üí business-directory (8005)     ‚îÇ
‚îÇ  ‚îú‚îÄ /ai/*        ‚Üí ai-agents (8008)              ‚îÇ
‚îÇ  ‚îú‚îÄ /trading/*   ‚Üí quanttrade (8009)             ‚îÇ
‚îÇ  ‚îú‚îÄ /sourcing/*  ‚Üí amazon-sourcing (8010)        ‚îÇ
‚îÇ  ‚îî‚îÄ /*           ‚Üí brain-gateway (8001)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Task 1.1: Configure DNS Records

**Cloudflare DNS Configuration:**

```bash
#!/bin/bash
# /home/alagiri/projects/bizosaas-platform/scripts/setup-dns-records.sh

CLOUDFLARE_API_TOKEN="8O_3_FRsFFbibnRQDeKpmRnqBz8WbrwWEEyy3H_g"
ZONE_ID="<bizoholic.com-zone-id>"  # Get from Cloudflare dashboard

# Function to add CNAME record
add_cname() {
  local subdomain=$1
  local target=$2

  echo "Adding CNAME: $subdomain ‚Üí $target"

  curl -X POST "https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records" \
    -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
    -H "Content-Type: application/json" \
    --data "{
      \"type\": \"CNAME\",
      \"name\": \"$subdomain\",
      \"content\": \"$target\",
      \"proxied\": true,
      \"ttl\": 1
    }" | jq '.'
}

# Add required subdomains
add_cname "api.bizoholic.com" "dk4.bizoholic.com"
add_cname "api.coreldove.com" "dk4.bizoholic.com"
add_cname "admin.bizoholic.com" "dk4.bizoholic.com"
add_cname "portal.bizoholic.com" "dk4.bizoholic.com"
add_cname "temporal.bizoholic.com" "dk4.bizoholic.com"
add_cname "vault.bizoholic.com" "dk4.bizoholic.com"
add_cname "superset.bizoholic.com" "dk4.bizoholic.com"

echo "‚úÖ DNS records configured"
echo "‚è∞ Waiting for DNS propagation (may take 5-10 minutes)..."
```

### Task 1.2: Deploy API Gateway Service

**Create API Gateway Application:**

```python
# /home/alagiri/projects/bizosaas-platform/backend/services/api-gateway/main.py

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import Response
from fastapi.middleware.cors import CORSMiddleware
import httpx
from typing import Dict
import logging

app = FastAPI(
    title="BizOSaaS API Gateway",
    version="1.0.0",
    description="Unified API Gateway for BizOSaaS Platform"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://stg.bizoholic.com",
        "https://stg.coreldove.com",
        "https://stg.thrillring.com",
        "https://portal.bizoholic.com",
        "https://admin.bizoholic.com",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Service Registry (DDD Bounded Contexts)
SERVICE_REGISTRY: Dict[str, str] = {
    "/auth": "http://backendservices-authservice-ux07ss:8002",
    "/crm": "http://backend-django-crm:8003",
    "/cms": "http://backend-wagtail-cms:8004",
    "/directory": "http://backend-business-directory:8005",
    "/ai": "http://backend-ai-agents:8008",
    "/trading": "http://backend-quanttrade-backend:8009",
    "/sourcing": "http://backend-amazon-sourcing:8010",
}

# Default to Brain Gateway
DEFAULT_SERVICE = "http://backend-brain-gateway:8001"

logger = logging.getLogger(__name__)

@app.api_route("/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS"])
async def gateway_proxy(path: str, request: Request):
    """
    API Gateway - Routes requests to appropriate microservices
    Implements DDD bounded context routing with circuit breaker pattern
    """

    # Determine target service from path
    target_service = DEFAULT_SERVICE
    route_prefix = None

    for prefix, service_url in SERVICE_REGISTRY.items():
        if f"/{path}".startswith(prefix):
            target_service = service_url
            route_prefix = prefix
            break

    # Remove route prefix from path for backend
    if route_prefix:
        backend_path = path[len(route_prefix.lstrip('/')):]
    else:
        backend_path = path

    # Build target URL
    target_url = f"{target_service}/{backend_path}"

    logger.info(f"Routing {request.method} /{path} ‚Üí {target_url}")

    # Forward request to backend service
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Prepare headers (remove host header)
            headers = dict(request.headers)
            headers.pop('host', None)

            # Forward request
            response = await client.request(
                method=request.method,
                url=target_url,
                headers=headers,
                params=request.query_params,
                content=await request.body()
            )

            # Return response
            return Response(
                content=response.content,
                status_code=response.status_code,
                headers=dict(response.headers),
                media_type=response.headers.get('content-type')
            )

    except httpx.TimeoutException:
        logger.error(f"Timeout calling {target_url}")
        raise HTTPException(status_code=504, detail="Gateway timeout")
    except httpx.ConnectError:
        logger.error(f"Connection error to {target_url}")
        raise HTTPException(status_code=503, detail="Service unavailable")
    except Exception as e:
        logger.error(f"Error proxying request: {e}")
        raise HTTPException(status_code=500, detail="Internal gateway error")

@app.get("/health")
async def health_check():
    """Gateway health check"""
    return {
        "status": "healthy",
        "service": "api-gateway",
        "version": "1.0.0",
        "services": len(SERVICE_REGISTRY)
    }

@app.get("/gateway/routes")
async def list_routes():
    """List all registered service routes"""
    return {
        "routes": [
            {
                "prefix": prefix,
                "service": url,
                "description": f"Routes {prefix}/* to {url}"
            }
            for prefix, url in SERVICE_REGISTRY.items()
        ],
        "default": DEFAULT_SERVICE
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
```

**Dockerfile:**

```dockerfile
# /home/alagiri/projects/bizosaas-platform/backend/services/api-gateway/Dockerfile

FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY main.py .

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:8080/health')"

# Expose port
EXPOSE 8080

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080", "--workers", "4"]
```

**requirements.txt:**

```
fastapi==0.104.1
uvicorn[standard]==0.24.0
httpx==0.25.1
python-multipart==0.0.6
```

**Build and Deploy:**

```bash
#!/bin/bash
# /home/alagiri/projects/bizosaas-platform/scripts/deploy-api-gateway.sh

cd /home/alagiri/projects/bizosaas-platform/backend/services/api-gateway

# Build image
docker build -t ghcr.io/bizoholic-digital/bizosaas-api-gateway:latest .

# Push to registry
docker push ghcr.io/bizoholic-digital/bizosaas-api-gateway:latest

# Deploy to KVM4
ssh root@72.60.219.244 << 'EOF'

  # Create API Gateway service
  docker service create \
    --name api-gateway \
    --network dokploy-network \
    --replicas 2 \
    --publish 8080:8080 \
    --with-registry-auth \
    --constraint 'node.role==manager' \
    --update-parallelism 1 \
    --update-delay 10s \
    --label "traefik.enable=true" \
    --label "traefik.http.routers.api-gateway.rule=Host(\`api.bizoholic.com\`)" \
    --label "traefik.http.routers.api-gateway.entrypoints=websecure" \
    --label "traefik.http.routers.api-gateway.tls.certresolver=letsencrypt" \
    --label "traefik.http.services.api-gateway.loadbalancer.server.port=8080" \
    ghcr.io/bizoholic-digital/bizosaas-api-gateway:latest

  echo "‚úÖ API Gateway deployed"
EOF
```

### Task 1.3: Configure Backend Service Domains

**Update Backend Services with Traefik Labels:**

```bash
#!/bin/bash
# /home/alagiri/projects/bizosaas-platform/scripts/configure-backend-routing.sh

ssh root@72.60.219.244 << 'EOF'

# Function to update service with Traefik labels
update_service() {
  local service=$1
  local hostname=$2
  local path_prefix=$3
  local port=$4

  echo "Configuring $service ‚Üí $hostname$path_prefix (port $port)"

  docker service update \
    --label-add "traefik.enable=true" \
    --label-add "traefik.http.routers.${service}.rule=Host(\`${hostname}\`) && PathPrefix(\`${path_prefix}\`)" \
    --label-add "traefik.http.routers.${service}.entrypoints=websecure" \
    --label-add "traefik.http.routers.${service}.tls.certresolver=letsencrypt" \
    --label-add "traefik.http.middlewares.${service}-strip.stripprefix.prefixes=${path_prefix}" \
    --label-add "traefik.http.routers.${service}.middlewares=${service}-strip@docker" \
    --label-add "traefik.http.services.${service}.loadbalancer.server.port=${port}" \
    $service
}

# Configure all backend services

# 1. Brain Gateway (default catch-all)
update_service "backend-brain-gateway" "api.bizoholic.com" "/" "8001"

# 2. Auth Service
update_service "backendservices-authservice-ux07ss" "api.bizoholic.com" "/auth" "8002"

# 3. Django CRM
update_service "backend-django-crm" "api.bizoholic.com" "/crm" "8003"

# 4. Wagtail CMS
update_service "backend-wagtail-cms" "api.bizoholic.com" "/cms" "8004"

# 5. Business Directory Backend
update_service "backend-business-directory" "api.bizoholic.com" "/directory" "8005"

# 6. AI Agents Service
update_service "backend-ai-agents" "api.bizoholic.com" "/ai" "8008"

# 7. QuantTrade Backend
update_service "backend-quanttrade-backend" "api.bizoholic.com" "/trading" "8009"

# 8. Amazon Sourcing
update_service "backend-amazon-sourcing" "api.bizoholic.com" "/sourcing" "8010"

# CoreLDove Services (separate domain)

# 9. Saleor GraphQL API
update_service "backend-saleor-api" "api.coreldove.com" "/graphql" "8000"

# 10. CoreLDove Backend
update_service "backend-coreldove-backend" "api.coreldove.com" "/v1" "8006"

echo "‚úÖ All backend services configured with Traefik routing"

EOF
```

### Task 1.4: Verify API Gateway Routing

**Test Script:**

```bash
#!/bin/bash
# /home/alagiri/projects/bizosaas-platform/scripts/test-api-gateway.sh

echo "Testing API Gateway Routing..."

# Test 1: Gateway Health
echo -e "\n1. Testing Gateway Health..."
curl -s https://api.bizoholic.com/health | jq '.'

# Test 2: List Routes
echo -e "\n2. Listing Gateway Routes..."
curl -s https://api.bizoholic.com/gateway/routes | jq '.'

# Test 3: Auth Service
echo -e "\n3. Testing Auth Service..."
curl -s https://api.bizoholic.com/auth/health | jq '.'

# Test 4: CRM Service
echo -e "\n4. Testing CRM Service..."
curl -s https://api.bizoholic.com/crm/health | jq '.'

# Test 5: CMS Service
echo -e "\n5. Testing CMS Service..."
curl -s https://api.bizoholic.com/cms/health | jq '.'

# Test 6: Saleor API (CoreLDove)
echo -e "\n6. Testing Saleor GraphQL..."
curl -s -X POST https://api.coreldove.com/graphql \
  -H "Content-Type: application/json" \
  -d '{"query":"{ shop { name } }"}' | jq '.'

echo -e "\n‚úÖ API Gateway testing complete"
```

### Success Criteria

- ‚úÖ DNS records propagated for `api.bizoholic.com` and `api.coreldove.com`
- ‚úÖ API Gateway service running (2 replicas)
- ‚úÖ SSL certificates auto-provisioned via Let's Encrypt
- ‚úÖ All 10 backend services accessible via API gateway
- ‚úÖ Health checks passing for all services
- ‚úÖ CORS properly configured
- ‚úÖ External API testing successful (Postman/curl)

### Time Estimate
**Duration:** 1-2 days (including DNS propagation wait time)

---

## PHASE 2: CrewAI Agents Orchestration (Days 5-8)

### Objective
Deploy event-driven infrastructure for 93+ CrewAI agents with HITL workflows

### Architecture: Agent Orchestration

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              BRAIN GATEWAY (8001)                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Request Classifier (AI-powered):                        ‚îÇ
‚îÇ  ‚îú‚îÄ Analyze incoming request                             ‚îÇ
‚îÇ  ‚îú‚îÄ Determine complexity & risk                          ‚îÇ
‚îÇ  ‚îú‚îÄ Route to: AUTO queue OR HITL queue                   ‚îÇ
‚îÇ  ‚îî‚îÄ Publish to RabbitMQ                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚Üì                    ‚Üì                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RABBITMQ    ‚îÇ   ‚îÇ REDIS STREAMS  ‚îÇ   ‚îÇ    KAFKA    ‚îÇ
‚îÇ Task Queues  ‚îÇ   ‚îÇ  Real-time     ‚îÇ   ‚îÇ   Events    ‚îÇ
‚îÇ              ‚îÇ   ‚îÇ  Coordination  ‚îÇ   ‚îÇ  Streaming  ‚îÇ
‚îÇ AUTO:        ‚îÇ   ‚îÇ                ‚îÇ   ‚îÇ             ‚îÇ
‚îÇ - orders     ‚îÇ   ‚îÇ - Agent status ‚îÇ   ‚îÇ - Decisions ‚îÇ
‚îÇ - support    ‚îÇ   ‚îÇ - HITL queue   ‚îÇ   ‚îÇ - Learning  ‚îÇ
‚îÇ - inventory  ‚îÇ   ‚îÇ - Metrics      ‚îÇ   ‚îÇ - Audit     ‚îÇ
‚îÇ              ‚îÇ   ‚îÇ                ‚îÇ   ‚îÇ             ‚îÇ
‚îÇ HITL:        ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ - approval   ‚îÇ
‚îÇ - exceptions ‚îÇ
‚îÇ - training   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           93+ CREWAI AGENT WORKERS                       ‚îÇ
‚îÇ           (Distributed Across Worker Pool)               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Order Processing (√ó4)     Lead Scoring (√ó3)             ‚îÇ
‚îÇ  Support Tickets (√ó6)      Content Gen (√ó5)              ‚îÇ
‚îÇ  Inventory Mgmt (√ó3)       SEO Optimization (√ó10)        ‚îÇ
‚îÇ  Marketing (√ó4)            Social Media (√ó8)             ‚îÇ
‚îÇ  Analytics (√ó11)           ... (√ó39 more agents)         ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  HITL Coordinator (√ó1) ‚Üí Escalates to humans             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Task 2.1: Deploy RabbitMQ Cluster

```bash
#!/bin/bash
# /home/alagiri/projects/bizosaas-platform/scripts/deploy-rabbitmq.sh

ssh root@72.60.219.244 << 'EOF'

# Create RabbitMQ service with management UI
docker service create \
  --name rabbitmq \
  --network dokploy-network \
  --replicas 2 \
  --publish 5672:5672 \
  --publish 15672:15672 \
  --env RABBITMQ_DEFAULT_USER=admin \
  --env RABBITMQ_DEFAULT_PASS='BizOSaaS2025@RabbitMQ!Secure' \
  --env RABBITMQ_DEFAULT_VHOST=bizosaas \
  --env RABBITMQ_NODENAME=rabbit@rabbitmq \
  --mount type=volume,source=rabbitmq-data,target=/var/lib/rabbitmq \
  --label "traefik.enable=true" \
  --label "traefik.http.routers.rabbitmq.rule=Host(\`admin.bizoholic.com\`) && PathPrefix(\`/rabbitmq\`)" \
  --label "traefik.http.routers.rabbitmq.entrypoints=websecure" \
  --label "traefik.http.routers.rabbitmq.tls.certresolver=letsencrypt" \
  --label "traefik.http.services.rabbitmq.loadbalancer.server.port=15672" \
  rabbitmq:3.13-management

echo "‚úÖ RabbitMQ deployed"
echo "üåê Management UI: https://admin.bizoholic.com/rabbitmq"
echo "üîê Credentials: admin / BizOSaaS2025@RabbitMQ!Secure"

EOF
```

**Setup Agent Queues:**

```python
# /home/alagiri/projects/bizosaas-platform/ai/services/bizosaas-brain/setup_queues.py

import pika
import sys

# Connection parameters
connection = pika.BlockingConnection(
    pika.ConnectionParameters(
        host='rabbitmq',
        port=5672,
        virtual_host='bizosaas',
        credentials=pika.PlainCredentials('admin', 'BizOSaaS2025@RabbitMQ!Secure')
    )
)
channel = connection.channel()

# Auto-Processing Queues (AI handles automatically)
AUTO_QUEUES = [
    ('auto_orders', 10, 3600000),          # High priority, 1h TTL
    ('auto_support_tickets', 10, 7200000), # High priority, 2h TTL
    ('auto_inventory', 5, 1800000),        # Medium priority, 30min TTL
    ('auto_marketing', 5, 14400000),       # Medium priority, 4h TTL
    ('auto_seo', 5, 7200000),              # Medium priority, 2h TTL
    ('auto_social_media', 5, 3600000),     # Medium priority, 1h TTL
    ('auto_email_campaigns', 5, 7200000),  # Medium priority, 2h TTL
    ('auto_analytics', 3, 14400000),       # Low priority, 4h TTL
]

# HITL Queues (Human approval required)
HITL_QUEUES = [
    ('hitl_approval', 10, 86400000),    # Critical, 24h TTL
    ('hitl_exceptions', 10, 43200000),  # Critical, 12h TTL
    ('hitl_training', 5, 604800000),    # Training data, 7d TTL
]

print("Creating RabbitMQ queues for CrewAI agents...")

# Create Dead Letter Exchange for failed messages
channel.exchange_declare(exchange='dlx', exchange_type='topic', durable=True)

for queue_name, max_priority, ttl in AUTO_QUEUES + HITL_QUEUES:
    # Create queue with DLX and priority
    channel.queue_declare(
        queue=queue_name,
        durable=True,
        arguments={
            'x-message-ttl': ttl,
            'x-max-priority': max_priority,
            'x-dead-letter-exchange': 'dlx',
            'x-dead-letter-routing-key': f'{queue_name}.dlq'
        }
    )

    # Create corresponding dead letter queue
    channel.queue_declare(
        queue=f'{queue_name}.dlq',
        durable=True
    )

    # Bind DLQ to DLX
    channel.queue_bind(
        queue=f'{queue_name}.dlq',
        exchange='dlx',
        routing_key=f'{queue_name}.dlq'
    )

    print(f"‚úÖ Created queue: {queue_name} (priority: {max_priority}, TTL: {ttl/1000}s)")

connection.close()

print("\n‚úÖ All queues created successfully!")
print(f"Total AUTO queues: {len(AUTO_QUEUES)}")
print(f"Total HITL queues: {len(HITL_QUEUES)}")
```

### Task 2.2: Deploy Kafka for Event Streaming

```bash
#!/bin/bash
# /home/alagiri/projects/bizosaas-platform/scripts/deploy-kafka.sh

ssh root@72.60.219.244 << 'EOF'

# Step 1: Deploy Zookeeper (Kafka dependency)
docker service create \
  --name zookeeper \
  --network dokploy-network \
  --replicas 1 \
  --publish 2181:2181 \
  --env ZOOKEEPER_CLIENT_PORT=2181 \
  --env ZOOKEEPER_TICK_TIME=2000 \
  --mount type=volume,source=zookeeper-data,target=/var/lib/zookeeper/data \
  --mount type=volume,source=zookeeper-logs,target=/var/lib/zookeeper/log \
  confluentinc/cp-zookeeper:7.5.0

echo "‚è∞ Waiting for Zookeeper to be ready..."
sleep 30

# Step 2: Deploy Kafka
docker service create \
  --name kafka \
  --network dokploy-network \
  --replicas 2 \
  --publish 9092:9092 \
  --env KAFKA_BROKER_ID=1 \
  --env KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
  --env KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
  --env KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=2 \
  --env KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=2 \
  --env KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=2 \
  --env KAFKA_AUTO_CREATE_TOPICS_ENABLE=true \
  --mount type=volume,source=kafka-data,target=/var/lib/kafka/data \
  confluentinc/cp-kafka:7.5.0

echo "‚úÖ Kafka cluster deployed (2 brokers)"

EOF
```

**Create Kafka Topics:**

```python
# /home/alagiri/projects/bizosaas-platform/ai/services/bizosaas-brain/setup_kafka_topics.py

from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError

admin_client = KafkaAdminClient(
    bootstrap_servers='kafka:9092',
    client_id='bizosaas-setup'
)

# Define topics for event streaming
topics = [
    # Domain Events (from services)
    NewTopic('domain.orders', num_partitions=3, replication_factor=2),
    NewTopic('domain.customers', num_partitions=3, replication_factor=2),
    NewTopic('domain.products', num_partitions=3, replication_factor=2),
    NewTopic('domain.leads', num_partitions=2, replication_factor=2),
    NewTopic('domain.content', num_partitions=2, replication_factor=2),

    # AI Agent Events
    NewTopic('ai.decisions', num_partitions=3, replication_factor=2),
    NewTopic('ai.completions', num_partitions=3, replication_factor=2),
    NewTopic('ai.errors', num_partitions=2, replication_factor=2),

    # HITL Events
    NewTopic('hitl.requests', num_partitions=2, replication_factor=2),
    NewTopic('hitl.decisions', num_partitions=2, replication_factor=2),
    NewTopic('hitl.feedback', num_partitions=2, replication_factor=2),

    # Audit & Analytics
    NewTopic('audit.trail', num_partitions=5, replication_factor=2),
    NewTopic('analytics.metrics', num_partitions=3, replication_factor=2),
]

print("Creating Kafka topics...")

try:
    admin_client.create_topics(new_topics=topics, validate_only=False)
    print(f"‚úÖ Created {len(topics)} Kafka topics")

    for topic in topics:
        print(f"  - {topic.name} ({topic.num_partitions} partitions, RF={topic.replication_factor})")

except TopicAlreadyExistsError:
    print("‚ö†Ô∏è Some topics already exist, skipping...")

admin_client.close()
```

### Task 2.3: Implement Agent Workers

**Base Agent Worker Class:**

```python
# /home/alagiri/projects/bizosaas-platform/ai/services/bizosaas-brain/workers/base_worker.py

import pika
import json
import logging
from crewai import Agent, Task, Crew
from typing import Callable, Dict, Any
from datetime import datetime

logger = logging.getLogger(__name__)

class CrewAIWorker:
    """
    Base class for CrewAI agent workers
    Consumes tasks from RabbitMQ, processes with CrewAI, publishes results
    """

    def __init__(self, queue_name: str, agent: Agent, result_handler: Callable = None):
        self.queue_name = queue_name
        self.agent = agent
        self.result_handler = result_handler
        self.connection = None
        self.channel = None
        self._connect()

    def _connect(self):
        """Establish RabbitMQ connection"""
        try:
            self.connection = pika.BlockingConnection(
                pika.ConnectionParameters(
                    host='rabbitmq',
                    port=5672,
                    virtual_host='bizosaas',
                    credentials=pika.PlainCredentials('admin', 'BizOSaaS2025@RabbitMQ!Secure'),
                    heartbeat=600,
                    blocked_connection_timeout=300
                )
            )
            self.channel = self.connection.channel()
            self.channel.basic_qos(prefetch_count=1)
            logger.info(f"‚úÖ Connected to RabbitMQ queue: {self.queue_name}")
        except Exception as e:
            logger.error(f"‚ùå Failed to connect to RabbitMQ: {e}")
            raise

    def process_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process task with CrewAI agent
        Override this method for custom processing logic
        """
        start_time = datetime.now()

        try:
            # Create CrewAI task
            task = Task(
                description=task_data.get('description', 'Process task'),
                agent=self.agent,
                expected_output=task_data.get('expected_output', 'Task result')
            )

            # Execute with CrewAI
            crew = Crew(agents=[self.agent], tasks=[task], verbose=True)
            result = crew.kickoff()

            execution_time = (datetime.now() - start_time).total_seconds()

            return {
                'status': 'completed',
                'result': str(result),
                'execution_time': execution_time,
                'agent': self.agent.role,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"‚ùå Task processing failed: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

    def callback(self, ch, method, properties, body):
        """RabbitMQ message callback"""
        try:
            # Parse task data
            task_data = json.loads(body)
            task_id = task_data.get('id', 'unknown')

            logger.info(f"üì• [{self.queue_name}] Received task: {task_id}")

            # Process task
            result = self.process_task(task_data)

            # Handle result
            if self.result_handler:
                self.result_handler(task_id, result)

            # Acknowledge message
            ch.basic_ack(delivery_tag=method.delivery_tag)

            logger.info(f"‚úÖ [{self.queue_name}] Task completed: {task_id} (status: {result['status']})")

        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Invalid JSON: {e}")
            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
        except Exception as e:
            logger.error(f"‚ùå Error processing task: {e}")
            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)

    def start(self):
        """Start consuming messages from queue"""
        logger.info(f"üéß [{self.queue_name}] Agent worker started. Waiting for tasks...")
        logger.info(f"ü§ñ Agent Role: {self.agent.role}")
        logger.info(f"üéØ Agent Goal: {self.agent.goal}")

        try:
            self.channel.basic_consume(
                queue=self.queue_name,
                on_message_callback=self.callback
            )
            self.channel.start_consuming()
        except KeyboardInterrupt:
            logger.info("Stopping worker...")
            self.channel.stop_consuming()
        finally:
            if self.connection and not self.connection.is_closed:
                self.connection.close()
                logger.info("Connection closed")
```

**Example: Order Processing Agent:**

```python
# /home/alagiri/projects/bizosaas-platform/ai/services/bizosaas-brain/workers/order_agent.py

from crewai import Agent
from workers.base_worker import CrewAIWorker
import logging

logging.basicConfig(level=logging.INFO)

# Define CrewAI Agent for Order Processing
order_agent = Agent(
    role='E-commerce Order Processing Specialist',
    goal='Process e-commerce orders efficiently with 95% automation rate',
    backstory="""You are an expert in order fulfillment, payment validation,
    fraud detection, and inventory management. You can automatically process
    most orders, escalating only high-risk or exceptional cases to humans.

    Your expertise includes:
    - Payment validation and fraud detection
    - Inventory verification and allocation
    - Shipping method optimization
    - Customer communication
    - Order status tracking
    """,
    verbose=True,
    allow_delegation=False
)

def handle_order_result(task_id: str, result: dict):
    """Handle completed order processing result"""
    if result['status'] == 'completed':
        # Publish to Kafka for analytics
        # Update order status in database
        # Send confirmation email to customer
        logging.info(f"Order {task_id} processed successfully")
    else:
        # Log error
        # Create HITL request if needed
        logging.error(f"Order {task_id} failed: {result.get('error')}")

# Create worker instance
worker = CrewAIWorker(
    queue_name='auto_orders',
    agent=order_agent,
    result_handler=handle_order_result
)

if __name__ == '__main__':
    worker.start()
```

**Deploy Agent Workers:**

```bash
#!/bin/bash
# /home/alagiri/projects/bizosaas-platform/scripts/deploy-agent-workers.sh

cd /home/alagiri/projects/bizosaas-platform/ai/services/bizosaas-brain

# Build agent workers image
docker build -t ghcr.io/bizoholic-digital/bizosaas-agent-workers:latest \
  -f Dockerfile.workers .

# Push to registry
docker push ghcr.io/bizoholic-digital/bizosaas-agent-workers:latest

# Deploy to KVM4
ssh root@72.60.219.244 << 'EOF'

# Deploy Order Processing Agents (4 replicas)
docker service create \
  --name agent-orders \
  --network dokploy-network \
  --replicas 4 \
  --with-registry-auth \
  --env QUEUE_NAME=auto_orders \
  --env RABBITMQ_HOST=rabbitmq \
  --env RABBITMQ_USER=admin \
  --env RABBITMQ_PASS='BizOSaaS2025@RabbitMQ!Secure' \
  --env OPENAI_API_KEY=sk-proj-... \
  --env OPENROUTER_API_KEY=sk-or-v1-... \
  ghcr.io/bizoholic-digital/bizosaas-agent-workers:latest \
  python workers/order_agent.py

# Deploy Support Agents (6 replicas)
docker service create \
  --name agent-support \
  --network dokploy-network \
  --replicas 6 \
  --with-registry-auth \
  --env QUEUE_NAME=auto_support_tickets \
  --env RABBITMQ_HOST=rabbitmq \
  --env RABBITMQ_USER=admin \
  --env RABBITMQ_PASS='BizOSaaS2025@RabbitMQ!Secure' \
  --env OPENAI_API_KEY=sk-proj-... \
  ghcr.io/bizoholic-digital/bizosaas-agent-workers:latest \
  python workers/support_agent.py

# Deploy Marketing Agents (4 replicas)
docker service create \
  --name agent-marketing \
  --network dokploy-network \
  --replicas 4 \
  --with-registry-auth \
  --env QUEUE_NAME=auto_marketing \
  --env RABBITMQ_HOST=rabbitmq \
  --env RABBITMQ_USER=admin \
  --env RABBITMQ_PASS='BizOSaaS2025@RabbitMQ!Secure' \
  --env OPENAI_API_KEY=sk-proj-... \
  ghcr.io/bizoholic-digital/bizosaas-agent-workers:latest \
  python workers/marketing_agent.py

echo "‚úÖ Agent workers deployed"
echo "Total workers: 14 (4 orders + 6 support + 4 marketing)"

EOF
```

### Task 2.4: Implement HITL Workflow

**HITL Decision Engine:**

```python
# /home/alagiri/projects/bizosaas-platform/ai/services/bizosaas-brain/hitl/decision_engine.py

from typing import Dict, Any
from enum import Enum

class HITLTrigger(Enum):
    """Reasons for requiring human approval"""
    HIGH_VALUE = "high_value_transaction"
    FRAUD_RISK = "fraud_risk_detected"
    NEGATIVE_SENTIMENT = "negative_customer_sentiment"
    UNUSUAL_PATTERN = "unusual_behavior_pattern"
    BRAND_CRITICAL = "brand_critical_content"
    LEGAL_REVIEW = "legal_review_required"
    NEW_CUSTOMER = "new_customer_high_risk"
    POLICY_VIOLATION = "potential_policy_violation"

class HITLDecisionEngine:
    """
    Determines if AI can auto-process or human approval needed
    Implements business rules for HITL escalation
    """

    # Thresholds
    HIGH_VALUE_THRESHOLD = 5000.00
    FRAUD_SCORE_THRESHOLD = 0.7
    NEGATIVE_SENTIMENT_THRESHOLD = -0.8
    COST_IMPACT_THRESHOLD = 10000.00

    @classmethod
    def should_escalate(cls, task_type: str, task_data: Dict[str, Any]) -> tuple[bool, list[HITLTrigger]]:
        """
        Determine if task should be escalated to HITL
        Returns: (should_escalate: bool, reasons: list[HITLTrigger])
        """
        triggers = []

        # Order processing rules
        if task_type == 'order':
            if task_data.get('amount', 0) > cls.HIGH_VALUE_THRESHOLD:
                triggers.append(HITLTrigger.HIGH_VALUE)

            if task_data.get('fraud_score', 0) > cls.FRAUD_SCORE_THRESHOLD:
                triggers.append(HITLTrigger.FRAUD_RISK)

            if task_data.get('customer_new', False) and task_data.get('amount', 0) > 1000:
                triggers.append(HITLTrigger.NEW_CUSTOMER)

        # Support ticket rules
        elif task_type == 'support_ticket':
            if task_data.get('sentiment', 0) < cls.NEGATIVE_SENTIMENT_THRESHOLD:
                triggers.append(HITLTrigger.NEGATIVE_SENTIMENT)

            keywords = ['refund', 'lawyer', 'lawsuit', 'fraud', 'scam', 'terrible']
            message = task_data.get('message', '').lower()
            if any(word in message for word in keywords):
                triggers.append(HITLTrigger.LEGAL_REVIEW)

        # Content creation rules
        elif task_type == 'content':
            if task_data.get('is_brand_critical', False):
                triggers.append(HITLTrigger.BRAND_CRITICAL)

            if task_data.get('legal_review_required', False):
                triggers.append(HITLTrigger.LEGAL_REVIEW)

        # Inventory management rules
        elif task_type == 'inventory':
            if task_data.get('unusual_demand', False):
                triggers.append(HITLTrigger.UNUSUAL_PATTERN)

            if task_data.get('cost_impact', 0) > cls.COST_IMPACT_THRESHOLD:
                triggers.append(HITLTrigger.HIGH_VALUE)

        return (len(triggers) > 0, triggers)

    @classmethod
    def generate_hitl_request(cls, task_type: str, task_data: Dict[str, Any],
                             triggers: list[HITLTrigger], ai_recommendation: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate HITL request with context for human reviewer
        """
        return {
            'id': task_data.get('id'),
            'task_type': task_type,
            'created_at': datetime.now().isoformat(),
            'urgency': cls._calculate_urgency(triggers),
            'triggers': [t.value for t in triggers],
            'task_data': task_data,
            'ai_recommendation': ai_recommendation,
            'ai_confidence': ai_recommendation.get('confidence', 0),
            'estimated_impact': cls._estimate_impact(task_data),
            'context': cls._build_context(task_type, task_data),
        }

    @staticmethod
    def _calculate_urgency(triggers: list[HITLTrigger]) -> str:
        """Calculate urgency level based on triggers"""
        high_urgency_triggers = [
            HITLTrigger.FRAUD_RISK,
            HITLTrigger.LEGAL_REVIEW,
            HITLTrigger.NEGATIVE_SENTIMENT
        ]

        if any(t in triggers for t in high_urgency_triggers):
            return 'high'
        elif len(triggers) >= 2:
            return 'medium'
        else:
            return 'low'

    @staticmethod
    def _estimate_impact(task_data: Dict[str, Any]) -> Dict[str, Any]:
        """Estimate business impact"""
        return {
            'financial': task_data.get('amount', 0),
            'customer_satisfaction': 'high' if task_data.get('vip_customer') else 'medium',
            'brand_reputation': 'high' if task_data.get('is_brand_critical') else 'low'
        }

    @staticmethod
    def _build_context(task_type: str, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """Build context for human reviewer"""
        context = {
            'customer_history': task_data.get('customer_history', {}),
            'related_orders': task_data.get('related_orders', []),
            'account_age_days': task_data.get('account_age_days', 0),
        }

        if task_type == 'order':
            context['payment_method'] = task_data.get('payment_method')
            context['shipping_address'] = task_data.get('shipping_address')
            context['billing_address'] = task_data.get('billing_address')

        return context
```

**HITL API Endpoints:**

```python
# /home/alagiri/projects/bizosaas-platform/backend/services/brain/hitl_api.py

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime
import redis
import json

router = APIRouter(prefix="/hitl", tags=["Human-In-The-Loop"])

# Redis for real-time HITL queue
redis_client = redis.Redis(
    host='infrastructureservices-bizosaasredis-w0gw3g',
    port=6379,
    db=1,  # Use DB 1 for HITL
    password='BizOSaaS2025@redis',
    decode_responses=True
)

class HITLRequest(BaseModel):
    id: str
    task_type: str
    urgency: str
    triggers: List[str]
    ai_recommendation: dict
    ai_confidence: float
    task_data: dict
    context: dict
    created_at: str

class HITLDecision(BaseModel):
    request_id: str
    decision: str  # 'approve', 'reject', 'modify'
    human_reviewer: str
    notes: str
    modified_action: Optional[dict] = None

@router.get("/pending", response_model=List[HITLRequest])
async def get_pending_requests(urgency: Optional[str] = None):
    """
    Get all pending HITL approval requests
    Optionally filter by urgency level
    """
    pending_keys = redis_client.keys("hitl:pending:*")
    requests = []

    for key in pending_keys:
        data = redis_client.hgetall(key)

        # Parse JSON fields
        request = HITLRequest(
            id=data['id'],
            task_type=data['task_type'],
            urgency=data['urgency'],
            triggers=json.loads(data['triggers']),
            ai_recommendation=json.loads(data['ai_recommendation']),
            ai_confidence=float(data['ai_confidence']),
            task_data=json.loads(data['task_data']),
            context=json.loads(data['context']),
            created_at=data['created_at']
        )

        # Filter by urgency if specified
        if urgency is None or request.urgency == urgency:
            requests.append(request)

    # Sort by urgency and creation time
    urgency_order = {'high': 0, 'medium': 1, 'low': 2}
    requests.sort(key=lambda r: (urgency_order.get(r.urgency, 3), r.created_at))

    return requests

@router.post("/decision")
async def submit_decision(decision: HITLDecision):
    """
    Human submits approval/rejection decision
    AI learns from this decision via Kafka event
    """
    request_key = f"hitl:pending:{decision.request_id}"

    if not redis_client.exists(request_key):
        raise HTTPException(status_code=404, detail="HITL request not found")

    # Get original request data
    original_data = redis_client.hgetall(request_key)

    # Move to completed with decision
    completed_key = f"hitl:completed:{decision.request_id}"
    redis_client.hset(completed_key, mapping={
        **original_data,
        'decision': decision.decision,
        'human_reviewer': decision.human_reviewer,
        'notes': decision.notes,
        'modified_action': json.dumps(decision.modified_action) if decision.modified_action else '',
        'decided_at': datetime.now().isoformat()
    })

    # Remove from pending
    redis_client.delete(request_key)

    # Set expiry on completed (keep for 30 days)
    redis_client.expire(completed_key, 2592000)

    # Publish to Kafka for AI learning
    from kafka import KafkaProducer

    producer = KafkaProducer(
        bootstrap_servers='kafka:9092',
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )

    producer.send('hitl.decisions', value={
        'request_id': decision.request_id,
        'decision': decision.decision,
        'ai_recommendation': json.loads(original_data['ai_recommendation']),
        'ai_confidence': float(original_data['ai_confidence']),
        'human_reviewer': decision.human_reviewer,
        'notes': decision.notes,
        'agreement': decision.decision == 'approve',
        'timestamp': datetime.now().isoformat()
    })

    producer.flush()
    producer.close()

    # Execute action based on decision
    if decision.decision == 'approve':
        # Re-queue original task to auto queue
        pass
    elif decision.decision == 'modify':
        # Queue modified task
        pass
    elif decision.decision == 'reject':
        # Log rejection, notify stakeholders
        pass

    return {
        "status": "decision_recorded",
        "request_id": decision.request_id,
        "decision": decision.decision
    }

@router.get("/metrics")
async def get_hitl_metrics():
    """
    HITL workflow metrics and statistics
    """
    pending_count = len(redis_client.keys("hitl:pending:*"))
    completed_count = len(redis_client.keys("hitl:completed:*"))

    # Get urgency breakdown
    pending_keys = redis_client.keys("hitl:pending:*")
    urgency_breakdown = {'high': 0, 'medium': 0, 'low': 0}

    for key in pending_keys:
        urgency = redis_client.hget(key, 'urgency')
        urgency_breakdown[urgency] = urgency_breakdown.get(urgency, 0) + 1

    return {
        "pending_requests": pending_count,
        "completed_24h": completed_count,
        "urgency_breakdown": urgency_breakdown,
        "average_response_time": "15 minutes",  # Calculate from timestamps
        "approval_rate": 0.85,  # Calculate from decisions
        "automation_rate": 0.90  # 90% tasks auto-processed
    }

@router.get("/dashboard")
async def get_dashboard_stats():
    """
    Real-time dashboard statistics for HITL monitoring
    """
    return {
        "total_tasks_today": 1247,
        "auto_processed": 1122,
        "hitl_required": 125,
        "automation_rate": 0.90,
        "pending_high_priority": 3,
        "pending_medium_priority": 8,
        "pending_low_priority": 12,
        "avg_decision_time_minutes": 12,
        "top_triggers": [
            {"trigger": "high_value_transaction", "count": 45},
            {"trigger": "fraud_risk_detected", "count": 32},
            {"trigger": "negative_customer_sentiment", "count": 28}
        ]
    }
```

### Success Criteria

- ‚úÖ RabbitMQ cluster running (2 replicas) with management UI
- ‚úÖ 11 task queues created (8 auto + 3 HITL)
- ‚úÖ Kafka cluster running (2 brokers) with 13 topics
- ‚úÖ Agent workers deployed (14 initial workers)
- ‚úÖ HITL API endpoints functional
- ‚úÖ HITL dashboard showing real-time metrics
- ‚úÖ Event streaming to Kafka working
- ‚úÖ Dead letter queues configured for failed messages

### Time Estimate
**Duration:** 3-4 days (including testing and tuning)

---

## Success Metrics

### Platform Health Indicators

**Service Availability:**
- Target: 99.5% uptime for all critical services
- Monitoring: Health checks every 30 seconds
- Alerting: PagerDuty/Slack notifications on failures

**API Performance:**
- Target: p95 response time < 500ms
- Target: p99 response time < 2000ms
- Monitoring: Prometheus + Grafana dashboards

**Agent Automation:**
- Target: 90% of tasks auto-processed
- Target: 10% requiring HITL approval
- Monitoring: HITL dashboard metrics

**Deployment Frequency:**
- Target: 5+ deployments per day
- Target: Zero-downtime deployments
- Monitoring: GitHub Actions success rate

### Implementation Checklist

**Phase 0 - Day 1:** (üîÑ IN PROGRESS - 60% Complete)
- [x] Fix client portal redirect to `/portal/dashboard` (‚úÖ Code ready)
- [üîÑ] Deploy v2.2.14 (IN PROGRESS - building now)
- [ ] Verify sidebar visibility
- [ ] Verify PWA manifest loading
- [ ] Test login flow end-to-end

**Phase 1 - Days 2-4:** (‚úÖ COMPLETE - 100%)
- [x] Configure DNS for API subdomains (‚úÖ api.bizoholic.com configured)
- [x] Deploy API Gateway service (‚úÖ brain-gateway IS the API Gateway)
- [x] Add Traefik labels to backend services (‚úÖ All services routed)
- [x] Test external API access (‚úÖ Working via Traefik)
- [x] Verify SSL certificates (‚úÖ All services have SSL)

**Phase 2 - Days 5-8:** (‚úÖ COMPLETE - 100%)
- [x] Deploy RabbitMQ cluster (‚úÖ infrastructureservices-rabbitmq-gktndk-rabbitmq-1)
- [x] Create 22 task queues (‚úÖ 8 AUTO + 3 HITL + 11 DLQ queues created)
- [x] Deploy Kafka cluster (‚úÖ infrastructureservices-kafka-ill4q0-kafka-1)
- [x] Create 13 Kafka topics (‚úÖ All topics verified with 33 partitions)
- [x] Deploy agent workers (3 initial) (‚úÖ Order, Support, Marketing workers deployed)
- [x] Test RabbitMQ ‚Üí Worker flow (‚úÖ Message consumption verified)
- [x] Test Worker error handling (‚úÖ Graceful failure confirmed)
- [x] Verify infrastructure stability (‚úÖ All services healthy 1/1 replicas)

**Phase 3 - Days 9-11:** (üîÑ IN PROGRESS - 14% Complete)
- [x] Enhance Auth Service with JWT (‚úÖ FastAPI-Users with JWT + Cookie backends)
- [x] Implement multi-tenant support (‚úÖ Tenant model with full isolation)
- [x] Implement RBAC (‚úÖ 5 roles: SUPER_ADMIN, TENANT_ADMIN, USER, READONLY, AGENT)
- [x] OAuth 2.0 integration (‚úÖ Google, GitHub, Microsoft)
- [x] Session management (‚úÖ UserSession with security tracking)
- [x] Deploy auth service (‚úÖ backendservices-authservice-ux07ss at api.bizoholic.com/auth)
- [x] Integrate Client Portal frontend (‚úÖ AuthContext + tenant switching complete)
- [ ] Integrate Bizoholic Frontend with centralized auth
- [ ] Integrate BizOSaaS Admin with centralized auth
- [ ] Integrate Business Directory with centralized auth
- [ ] Integrate CoreLDove Frontend with centralized auth
- [ ] Integrate ThrillRing Gaming with centralized auth
- [ ] Integrate Analytics Dashboard with centralized auth
- [ ] Test SSO across all 7 platforms
- [ ] Verify tenant switching across platforms
- [ ] Test role-based permissions

**Phase 4 - Days 12-15:** (‚è≥ NOT STARTED - 0% Complete)
- [ ] Implement event bus
- [ ] Add circuit breakers
- [ ] Define 8 bounded contexts
- [ ] Set up inter-service events
- [ ] Test event-driven flows

**Phase 5 - Days 16-18:** (‚è≥ NOT STARTED - 0% Complete)
- [ ] Standardize Dockerfiles (22 services)
- [ ] Create GitHub Actions workflows
- [ ] Test automated deployments
- [ ] Configure health checks
- [ ] Verify zero-downtime deploys

**Phase 6 - Days 19-21:** (‚è≥ NOT STARTED - 0% Complete)
- [ ] Write integration tests
- [ ] Create API documentation
- [ ] Write developer guide
- [ ] Create operations runbook
- [ ] Conduct platform review

---

## Next Steps

### Immediate Actions (This Week)

1. **Review this implementation plan** with stakeholders
2. **Approve architecture decisions** (DDD, event-driven, etc.)
3. **Allocate development resources** (who works on what)
4. **Set up project tracking** (GitHub Projects, Jira, etc.)
5. **Begin Phase 0** - Fix critical client portal bug

### Short Term (Next 2 Weeks)

6. **Execute Phases 1-3**
   - API Gateway deployment
   - CrewAI orchestration
   - Centralized authentication

7. **Monitor progress daily**
   - Stand-ups to track blockers
   - Update project board
   - Adjust timeline as needed

### Long Term (Week 3)

8. **Complete Phases 4-6**
   - DDD boundaries
   - Containerization
   - Testing & docs

9. **Platform launch preparation**
   - Performance testing
   - Security audit
   - Customer onboarding

### Post-Implementation

10. **Monitor & Optimize**
    - Track KPIs (uptime, performance, automation rate)
    - Gather user feedback
    - Iterate on improvements

11. **Plan v2.0 Features**
    - Advanced AI agent capabilities
    - Additional integrations
    - Platform expansion

---

## Appendix

### Useful Commands

```bash
# Check service status
ssh root@72.60.219.244 'docker service ls'

# View service logs
ssh root@72.60.219.244 'docker service logs -f SERVICE_NAME'

# Scale service
ssh root@72.60.219.244 'docker service scale SERVICE_NAME=5'

# Update service image
ssh root@72.60.219.244 'docker service update --image NEW_IMAGE SERVICE_NAME'

# Check RabbitMQ queues
curl -u admin:PASSWORD https://admin.bizoholic.com/rabbitmq/api/queues

# Check Kafka topics
ssh root@72.60.219.244 'docker exec -it $(docker ps -q -f name=kafka) kafka-topics --list --bootstrap-server localhost:9092'
```

### Contact Information

**Platform Owner:** BizOSaaS Team
**DevOps Lead:** [TBD]
**Documentation:** https://docs.bizosaas.com
**Support:** support@bizosaas.com

---

**Document Status:** ‚úÖ APPROVED FOR IMPLEMENTATION
**Last Updated:** November 14, 2025
**Version:** 2.0 FINAL
