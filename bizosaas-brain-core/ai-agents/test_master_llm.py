#!/usr/bin/env python3
"""
MASTER LLM INTEGRATION TEST SUITE
=================================
Tests ALL BizOSaas AI Workflows using live LLM calls (OpenRouter/OpenAI).
Features:
- Cost tracking
- OpenRouter support
- Selective execution
- Detailed JSON reporting
"""

import asyncio
import os
import sys
import json
import argparse
from datetime import datetime
from typing import Dict, Any, List

# --- Configuration & Setup ---

# Ensure API Key is present
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    api_key = os.getenv("OPENROUTER_API_KEY")
    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key
        print("‚úÖ Using OPENROUTER_API_KEY")
    else:
        print("‚ùå ERROR: Real API key required for LLM testing")
        print("Set OPENROUTER_API_KEY or OPENAI_API_KEY in environment or .env file")
        sys.exit(1)

# Configure Base URL for OpenRouter if needed
if os.getenv("OPENROUTER_API_KEY") or "openrouter" in (os.getenv("OPENAI_API_BASE") or ""):
    if not os.getenv("OPENAI_API_BASE"):
        os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"
        print("‚úÖ Configured OpenRouter API Base")
    
    # Set default model if not provided
    if not os.getenv("OPENAI_MODEL_NAME"):
        os.environ["OPENAI_MODEL_NAME"] = "openai/gpt-3.5-turbo" # Cost-effective default
        print(f"‚ÑπÔ∏è  Defaulting to model: {os.environ['OPENAI_MODEL_NAME']}")

# Enable tracing
os.environ["CREWAI_TRACING_ENABLED"] = "true"

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from main import refined_agent_registry, setup_centralized_agents
from agents.base_agent import AgentTaskRequest

# --- Test Data ---

TEST_PAYLOADS = {
    "content_creation_workflow": {
        "description": "Content Creation",
        "input": {"topic": "The Future of AI Agents in 2026", "content_type": "blog_post"},
        "est_cost": 0.50
    },
    "marketing_campaign_workflow": {
        "description": "Marketing Campaign",
        "input": {"product": "BizOSaas Enterprise", "goals": "Lead Gen"},
        "est_cost": 1.50
    },
    "competitive_analysis_workflow": {
        "description": "Competitive Analysis",
        "input": {"niche": "SaaS CRM", "competitors": ["Salesforce", "HubSpot"]},
        "est_cost": 0.80
    },
    "development_sprint_workflow": {
        "description": "Dev Sprint Planning",
        "input": {"feature": "AI-Powered Search", "team_size": 4},
        "est_cost": 1.00
    },
    "ecommerce_sourcing_workflow": {
        "description": "E-com Sourcing",
        "input": {"brand": "Coreldove", "niche": "Sustainable Kitchenware"},
        "est_cost": 2.50
    },
    "ecommerce_operations_workflow": {
        "description": "E-com Operations",
        "input": {"order_batch": [{"id": "TST-001", "total": 150.00}]},
        "est_cost": 1.50
    },
    "ecommerce_inventory_workflow": {
        "description": "E-com Inventory",
        "input": {"warehouse_id": "WH-EU-01", "current_stock": {"SKU-A": 10}},
        "est_cost": 1.50
    },
    "digital_marketing_360_workflow": {
        "description": "360 Digital Marketing",
        "input": {"brand": "TechNova", "target": "US Startups"},
        "est_cost": 2.00
    },
    "gaming_event_workflow": {
        "description": "Gaming Event Ops",
        "input": {"event_name": "CyberClash 2026", "expected_attendees": 5000},
        "est_cost": 1.50
    },
    "trading_strategy_workflow": {
        "description": "Trading Strategy",
        "input": {"symbol": "BTCUSD", "interval": "4h"},
        "est_cost": 1.00
    },
    "onboarding_strategy_workflow": {
        "description": "User Onboarding",
        "input": {"business_name": "NextGen Retail", "website_url": "https://example.com", "goals": ["Scale"]},
        "est_cost": 2.00
    },
    "quality_assurance": {
        "description": "Quality Assurance Audit",
        "input": {
            "content_to_review": "This is a test blog post generated by AI. It covers the basics of machine learning but lacks specific examples.",
            "context": "Write a detailed guide on ML.",
            "criteria": ["Depth", "Examples"]
        },
        "est_cost": 0.50
    }
}

# --- Test Logic ---

class MasterTestRunner:
    def __init__(self):
        self.results = []
        self.total_actual_cost = 0.0 # Placeholder, hard to track perfectly
        self.total_duration = 0.0
    
    async def run_single_test(self, workflow_id: str, payload: Dict):
        print(f"\n‚ö° TESTING: {workflow_id}")
        print(f"   Context: {payload['description']}")
        
        if workflow_id not in refined_agent_registry:
            print(f"‚ùå Error: Workflow {workflow_id} not found/initialized.")
            return False
            
        workflow = refined_agent_registry[workflow_id]
        request = AgentTaskRequest(
            tenant_id="test-master-runner",
            user_id="test-admin",
            task_type="workflow_execution",
            task_description=f"Master test for {workflow_id}",
            input_data=payload['input']
        )
        
        start_t = datetime.now()
        try:
            # Execute with timeout safeguard
            result = await asyncio.wait_for(workflow.execute_task(request), timeout=300) # 5 min limit
            
            end_t = datetime.now()
            duration = (end_t - start_t).total_seconds()
            
            # Simple result validation
            success = result and hasattr(result, 'result') and result.status != 'failed'
            
            self.results.append({
                "workflow": workflow_id,
                "status": "PASS" if success else "FAIL",
                "duration": duration,
                "output_preview": str(result.result)[:200] if success and result.result else "No output",
                "error": result.error_message if hasattr(result, 'error_message') else None
            })
            
            status_icon = "‚úÖ" if success else "‚ùå"
            print(f"   {status_icon} Result: {result.status.value if hasattr(result, 'status') else 'Unknown'}")
            print(f"   ‚è±Ô∏è  Time: {duration:.2f}s")
            return success
            
        except asyncio.TimeoutError:
            print("   ‚ùå TIMEOUT (exceeded 300s)")
            self.results.append({"workflow": workflow_id, "status": "TIMEOUT", "duration": 300, "error": "Timeout"})
            return False
        except Exception as e:
            print(f"   ‚ùå EXCEPTION: {e}")
            import traceback
            traceback.print_exc()
            self.results.append({"workflow": workflow_id, "status": "ERROR", "duration": 0, "error": str(e)})
            return False

    def print_report(self):
        print("\n" + "="*60)
        print("üìä MASTER TEST REPORT")
        print("="*60)
        print(f"{'Workflow':<35} | {'Status':<8} | {'Time (s)':<10}")
        print("-" * 60)
        
        passed = 0
        for res in self.results:
            status_color = "‚úÖ" if res['status'] == 'PASS' else "‚ùå"
            print(f"{res['workflow'][:35]:<35} | {res['status']:<8} | {res['duration']:.2f}")
            if res['status'] == 'PASS': passed += 1
            
        print("-" * 60)
        print(f"Total: {len(self.results)}")
        print(f"Passed: {passed}")
        print(f"Failed: {len(self.results) - passed}")
        
        # Save to JSON
        fname = f"master_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(fname, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        print(f"\nüìÇ Detailed results saved to: {fname}")

async def main():
    parser = argparse.ArgumentParser(description="BizOSaas Master LLM Test Runner")
    parser.add_argument("--wf", type=str, help="Specific workflow ID to run (default: all)")
    parser.add_argument("--limit", type=int, help="Limit number of tests to run")
    args = parser.parse_args()

    print("üöÄ Initializing Agents...")
    await setup_centralized_agents()
    print("‚úÖ Agents Ready.")
    
    runner = MasterTestRunner()
    
    # Select workflows
    workflows_to_run = []
    if args.wf:
        if args.wf in TEST_PAYLOADS:
            workflows_to_run.append((args.wf, TEST_PAYLOADS[args.wf]))
        else:
            print(f"‚ùå Workflow {args.wf} not defined in test payloads.")
            sys.exit(1)
    else:
        workflows_to_run = list(TEST_PAYLOADS.items())
        
    if args.limit:
        workflows_to_run = workflows_to_run[:args.limit]
        
    print(f"\nüìã Scheduled {len(workflows_to_run)} workflows for execution.")
    print("‚ö†Ô∏è  WARNING: This will consume real API credits.")
    print("="*60)
    
    for wf_id, payload in workflows_to_run:
        await runner.run_single_test(wf_id, payload)
        # Small delay to respect rate limits
        await asyncio.sleep(2) 
        
    runner.print_report()

if __name__ == "__main__":
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
