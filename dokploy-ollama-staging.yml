services:
  # ==================================================
  # OLLAMA - Local LLM Engine
  # ==================================================
  ollama:
    image: ollama/ollama:latest
    container_name: bizosaas-ollama-staging
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      # Allow CORS for web UI access
      - OLLAMA_ORIGINS=*
    networks:
      - dokploy-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ollama-api.rule=Host(`ollama-api.bizoholic.net`)"
      - "traefik.http.routers.ollama-api.entrypoints=websecure"
      - "traefik.http.routers.ollama-api.tls.certresolver=letsencrypt"
      - "traefik.http.services.ollama-api.loadbalancer.server.port=11434"
    deploy:
      resources:
        reservations:
          memory: 2G
        limits:
          memory: 8G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ==================================================
  # OPEN WEBUI - Feature-Rich Ollama Interface
  # ==================================================
  # Recommended for testing and feature exploration
  # Features: RAG, Document Processing, Multi-modal, Model Management
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: bizosaas-open-webui-staging
    restart: unless-stopped
    ports:
      - "3012:8080"
    environment:
      # Ollama Connection
      - OLLAMA_BASE_URL=http://bizosaas-ollama-staging:11434
      
      # Security
      - WEBUI_SECRET_KEY=bizosaas-openwebui-secret-2025-staging
      
      # Features
      - ENABLE_RAG_WEB_SEARCH=true
      - ENABLE_IMAGE_GENERATION=false
      - ENABLE_COMMUNITY_SHARING=false
      
      # Authentication (optional - enable for multi-user)
      - WEBUI_AUTH=false
      
      # Model Management
      - DEFAULT_MODELS=llama2
      - DEFAULT_USER_ROLE=admin
      
    volumes:
      - open_webui_data:/app/backend/data
    networks:
      - dokploy-network
    depends_on:
      - ollama
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.open-webui.rule=Host(`ai.bizoholic.net`)"
      - "traefik.http.routers.open-webui.entrypoints=websecure"
      - "traefik.http.routers.open-webui.tls.certresolver=letsencrypt"
      - "traefik.http.services.open-webui.loadbalancer.server.port=8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  ollama_data:
    driver: local
  open_webui_data:
    driver: local

networks:
  dokploy-network:
    external: true

# ==================================================
# DEPLOYMENT SUMMARY
# ==================================================
# Project: bizosaas-ai
# Services: 2 (Ollama + Open WebUI)
# 
# DOMAINS REQUIRED:
#   - ollama-api.bizoholic.net (API access)
#   - ai.bizoholic.net (Web UI)
#
# FEATURES:
#   ✅ Native Ollama integration
#   ✅ Document upload & RAG (PDF, DOC, TXT)
#   ✅ Multi-modal support (images)
#   ✅ Model management (download, switch, create)
#   ✅ Python function calling
#   ✅ Web search integration
#   ✅ Conversation history
#   ✅ Export/Import chats
#
# RECOMMENDED MODELS:
#   - llama2 (7B) - General purpose, good balance
#   - phi (2.7B) - Fast, lightweight testing
#   - mistral (7B) - High quality responses
#   - codellama (7B) - Code generation
#
# DOWNLOAD MODELS:
#   docker exec bizosaas-ollama-staging ollama pull llama2
#   docker exec bizosaas-ollama-staging ollama pull phi
#   docker exec bizosaas-ollama-staging ollama pull mistral
#
# WHY OPEN WEBUI?
#   1. Native Ollama integration (built specifically for it)
#   2. RAG support - upload docs for context-aware responses
#   3. Feature-rich - best for understanding capabilities
#   4. Active development - latest Ollama features
#   5. Perfect for replicating features in Client Portal
#
# ALTERNATIVES (not included, but available):
#   - LibreChat: Multi-provider (OpenAI, Anthropic, etc.) - more complex
#   - Open Fiesta: Multi-model comparison - good for testing different models
#
# ==================================================
